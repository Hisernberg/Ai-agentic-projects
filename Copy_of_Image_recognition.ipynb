{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hisernberg/Ai-agentic-projects/blob/main/Copy_of_Image_recognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh\n",
        "!nohup ollama serve > output.log 2>&1 &\n",
        "!ollama pull phi4-mini"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZMrmXlTfRk4H",
        "outputId": "c8d59489-95ab-4fe5-a93f-f4a91641f0e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Cleaning up old version at /usr/local/lib/ollama\n",
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "############################################################################################# 100.0%\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python pillow requests langchain langchain_community ollama langchain_ollama requests pillow torchvision torch transformers"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hh5tYwK0WwwC",
        "outputId": "42b88748-cd77-4a92-eecd-1c6938e91f1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.25)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.24-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting ollama\n",
            "  Downloading ollama-0.4.8-py3-none-any.whl.metadata (4.7 kB)\n",
            "Collecting langchain_ollama\n",
            "  Downloading langchain_ollama-0.3.3-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python) (2.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.58 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.59)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.4)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.9.1-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain_community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: httpx<0.29,>=0.27 in /usr/local/lib/python3.11/dist-packages (from ollama) (0.28.1)\n",
            "Collecting langchain-core<1.0.0,>=0.3.58 (from langchain)\n",
            "  Downloading langchain_core-0.3.60-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.27->ollama) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.27->ollama) (0.16.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.58->langchain) (1.33)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.58->langchain) (3.0.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.27->ollama) (1.3.1)\n",
            "Downloading langchain_community-0.3.24-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ollama-0.4.8-py3-none-any.whl (13 kB)\n",
            "Downloading langchain_ollama-0.3.3-py3-none-any.whl (21 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m71.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading langchain_core-0.3.60-py3-none-any.whl (437 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m437.9/437.9 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.9.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Installing collected packages: python-dotenv, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mypy-extensions, marshmallow, httpx-sse, typing-inspect, nvidia-cusparse-cu12, nvidia-cudnn-cu12, pydantic-settings, ollama, nvidia-cusolver-cu12, dataclasses-json, langchain-core, langchain_ollama, langchain_community\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 0.3.59\n",
            "    Uninstalling langchain-core-0.3.59:\n",
            "      Successfully uninstalled langchain-core-0.3.59\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-core-0.3.60 langchain_community-0.3.24 langchain_ollama-0.3.3 marshmallow-3.26.1 mypy-extensions-1.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ollama-0.4.8 pydantic-settings-2.9.1 python-dotenv-1.1.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ✅ Download Sample Images\n",
        "import requests\n",
        "from pathlib import Path\n",
        "from PIL import Image\n",
        "from torchvision import models, transforms\n",
        "import torch\n",
        "\n",
        "Path(\"sample_images\").mkdir(exist_ok=True)\n",
        "images = {\n",
        "    \"dog.jpg\": \"https://images.unsplash.com/photo-1518717758536-85ae29035b6d\",\n",
        "    \"cat.jpg\": \"https://images.unsplash.com/photo-1592194996308-7b43878e84a6\",\n",
        "    \"car.jpg\": \"https://images.unsplash.com/photo-1503376780353-7e6692767b70\"\n",
        "}\n",
        "for filename, url in images.items():\n",
        "    response = requests.get(url)\n",
        "    with open(f\"sample_images/{filename}\", \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "print(\"✅ Sample images downloaded.\")\n",
        "\n",
        "# ✅ Load Pre-trained Model\n",
        "model = models.resnet50(pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "LABELS_URL = \"https://raw.githubusercontent.com/pytorch/hub/master/imagenet_classes.txt\"\n",
        "labels = requests.get(LABELS_URL).text.strip().split(\"\\n\")\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "def recognize_image(image_path):\n",
        "    img = Image.open(image_path).convert(\"RGB\")\n",
        "    input_tensor = transform(img).unsqueeze(0)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_tensor)\n",
        "    _, predicted_idx = torch.max(outputs, 1)\n",
        "    return labels[predicted_idx.item()]\n",
        "\n",
        "# ✅ Setup LangChain with Ollama (Phi-4 Mini)\n",
        "from langchain_community.llms import Ollama\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = Ollama(model=\"phi4-mini\")  # Make sure Ollama is running locally with `phi4-mini` pulled\n",
        "template = \"\"\"\n",
        "You are an AI assistant helping with image recognition.\n",
        "The image content was identified as: {description}\n",
        "\n",
        "Please describe this object and suggest possible one short use cases.\n",
        "\n",
        "Answer:\n",
        "\"\"\"\n",
        "prompt = PromptTemplate.from_template(template)\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# ✅ Process Images\n",
        "for image_name in images:\n",
        "    image_path = f\"sample_images/{image_name}\"\n",
        "    label = recognize_image(image_path)\n",
        "    print(f\"\\n🖼️ Image: {image_name}\")\n",
        "    print(f\"📷 Recognized Label: {label}\")\n",
        "    response = chain.run({\"description\": label})\n",
        "    print(f\"🤖 Phi-4 Mini Response:\\n{response}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBuuc__Hf1dP",
        "outputId": "f0a67367-1a53-40d1-b3b3-cd3d781e6936"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Sample images downloaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🖼️ Image: dog.jpg\n",
            "📷 Recognized Label: German short-haired pointer\n",
            "🤖 Phi-4 Mini Response:\n",
            "A German Short-Haired Pointer is a medium-sized dog known for its wiry, dense coat that ranges from light to dark red-brown. This breed has strong hindquarters with well-muscled legs giving it an athletic build suitable mainly as hunting dogs due to their high energy levels and excellent tracking abilities.\n",
            "\n",
            "Possible use case: A German Short-Haired Pointer can be used in controlled environments for pest control, targeting rodents or raccoons that might invade agricultural areas.\n",
            "\n",
            "🖼️ Image: cat.jpg\n",
            "📷 Recognized Label: Persian cat\n",
            "🤖 Phi-4 Mini Response:\n",
            "A Persian Cat is a breed of domestic cats known for its long, luxurious fur that reaches down to the paws. It has distinct physical characteristics such as large eyes set far apart with almond-shaped pupils, an elongated body shape often described in terms like 'bat-like', and muscular build which contributes to their expressive facial features.\n",
            "\n",
            "Possible short use case: A Persian Cat can serve effectively at a pet photo shoot for luxury home decor websites due to its striking appearance.\n",
            "\n",
            "🖼️ Image: car.jpg\n",
            "📷 Recognized Label: sports car\n",
            "🤖 Phi-4 Mini Response:\n",
            "A sports car is typically characterized by its sleek, aerodynamic design that emphasizes speed. It usually features a low ground clearance for better handling on the road or track; powerful engines designed to maximize acceleration while maintaining some balance between power output at higher speeds (top-end performance) and lower revs efficiency around town.\n",
            "\n",
            "One short use case: A sports car can be used as an exciting means of transportation that provides thrilling drives through scenic routes, such as coastal highways ideal for racing enthusiasts.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -U"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 738
        },
        "id": "t2glG86O8dXR",
        "outputId": "9e8efcb5-228c-4ecd-9fea-9f40f6839744"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Collecting huggingface-hub<1.0,>=0.30.0 (from transformers)\n",
            "  Downloading huggingface_hub-0.31.4-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m102.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.31.4-py3-none-any.whl (489 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m489.3/489.3 kB\u001b[0m \u001b[31m35.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: huggingface-hub, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.24.7\n",
            "    Uninstalling huggingface-hub-0.24.7:\n",
            "      Successfully uninstalled huggingface-hub-0.24.7\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.48.3\n",
            "    Uninstalling transformers-4.48.3:\n",
            "      Successfully uninstalled transformers-4.48.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yolov5 7.0.14 requires huggingface-hub<0.25.0,>=0.12.0, but you have huggingface-hub 0.31.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed huggingface-hub-0.31.4 transformers-4.51.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "huggingface_hub"
                ]
              },
              "id": "41652dc58b854c359d04c68187b1085a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download sample images\n",
        "import requests\n",
        "from pathlib import Path\n",
        "Path(\"sample_images\").mkdir(exist_ok=True)\n",
        "\n",
        "images = {\n",
        "    \"dog.jpg\": \"https://images.unsplash.com/photo-1518717758536-85ae29035b6d\",\n",
        "    \"cat.jpg\": \"https://images.unsplash.com/photo-1592194996308-7b43878e84a6\",\n",
        "    \"car.jpg\": \"https://images.unsplash.com/photo-1503376780353-7e6692767b70\"\n",
        "}\n",
        "\n",
        "for filename, url in images.items():\n",
        "    response = requests.get(url)\n",
        "    with open(f\"sample_images/{filename}\", \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "print(\"✅ Sample images downloaded.\")\n",
        "\n",
        "# Load CLIP model\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "\n",
        "# Define candidate labels\n",
        "labels = [\"dog\", \"cat\", \"car\", \"tree\", \"person\", \"computer\", \"building\", \"bottle\", \"phone\", \"book\"]\n",
        "\n",
        "def recognize_clip(image_path):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = processor(text=labels, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
        "    outputs = model(**inputs)\n",
        "    logits_per_image = outputs.logits_per_image\n",
        "    probs = logits_per_image.softmax(dim=1)\n",
        "    predicted_index = probs.argmax().item()\n",
        "    return labels[predicted_index]\n",
        "\n",
        "# LangChain + Ollama (Requires local Ollama server with `phi4-mini`)\n",
        "from langchain_community.llms import Ollama\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = Ollama(model=\"phi4-mini\")  # Requires Ollama running locally\n",
        "prompt = PromptTemplate.from_template(\"\"\"\n",
        "You are an AI assistant helping with image recognition.\n",
        "The image content was identified as: {description}\n",
        "\n",
        "Please describe this object and suggest possible use cases.\n",
        "\n",
        "Answer:\n",
        "\"\"\")\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# Run recognition and generate response\n",
        "for image_name in images:\n",
        "    image_path = f\"sample_images/{image_name}\"\n",
        "    label = recognize_clip(image_path)\n",
        "    print(f\"\\n🖼️ Image: {image_name}\")\n",
        "    print(f\"📷 Recognized Label (CLIP): {label}\")\n",
        "    response = chain.run({\"description\": label})\n",
        "    print(f\"🤖 Phi-4 Mini Response:\\n{response}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3Jb1RFhzhHv",
        "outputId": "db96d1f4-8cf8-4f4d-c13e-243da2c0cc1d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Sample images downloaded.\n",
            "\n",
            "🖼️ Image: dog.jpg\n",
            "📷 Recognized Label (CLIP): dog\n",
            "🤖 Phi-4 Mini Response:\n",
            "A \"dog\" is a domesticated carnivorous mammal belonging to the Canidae family. Dogs typically have four legs, fur covering their bodies which can vary in color from white through shades of grey or black, with some having markings such as spots; they possess ears that are usually pointed and eyes positioned on either side at different heights depending upon breed (e.g., brachycephalic breeds like bulldogs will naturally tilt the head forward). Their noses have a wet nose area called the muzzle. Dogs also exhibit a wide range of behaviors due to their intelligence, such as barking or vocalizing for communication.\n",
            "\n",
            "Possible use cases/suggestions:\n",
            "\n",
            "1. **Companionship:** One primary role that dogs play is serving as pets and companions; they offer emotional support through affection which can reduce stress levels in humans.\n",
            "   \n",
            "2. **Security/Guardianship:** Dogs have been trained to work with law enforcement agencies or private security firms for protection services, search-and-rescue missions.\n",
            "\n",
            "3. **Service Work/Dogs of the Disabled**: They assist individuals who are disabled (blindness) by providing guidance and alerting them when obstacles in their path need navigating around; they also guide people through physical therapy sessions as well as serving at hospitals or helping elderly to get out for exercise.\n",
            "   \n",
            "4. **Hunting/Companionship Work:** Dogs have been used historically, particularly before the widespread availability of firearms, by humans hunting prey such as deer and rabbits.\n",
            "\n",
            "5. **Guard Animals/Grazing Assistance:** They work in agricultural settings where they help protect livestock from predators or assist with herding activities; their keen senses make them excellent guardians.\n",
            "   \n",
            "6. **Therapeutic Aides**: Dogs are often used for therapeutic purposes, including assisting individuals who have suffered a traumatic event such as PTSD (Post-Traumatic Stress Disorder), phobia disorders like agoraphobia and social anxiety disorder.\n",
            "\n",
            "7. **Entertainment/Performing Arts:** Many breeds of dogs participate in entertainment fields; they perform tricks or showcase their talents through obedience competitions for audiences to enjoy.\n",
            "   \n",
            "8. **Search-and-Rescue Missions**: Dogs are trained extensively by government agencies, as well as private search teams used worldwide and nationally across the US.\n",
            "\n",
            "9. **Medical Assistance/Assistance with Disabilities:** Certain breeds of dogs have been specially bred over time that possess medical benefits; they can assist individuals who suffer from various diseases or disabilities (e.g., epilepsy) through companionship.\n",
            "10. **Family Members/Familial Support Role**: Dogs are increasingly being seen as family members themselves, providing a sense of responsibility and nurturing for owners.\n",
            "\n",
            "Dogs serve countless useful purposes in today's society ranging widely across different fields such as agriculture to security services; there can be numerous applications based on the breed's characteristics or an individual's requirements.\n",
            "\n",
            "🖼️ Image: cat.jpg\n",
            "📷 Recognized Label (CLIP): cat\n",
            "🤖 Phi-4 Mini Response:\n",
            "This is a common household pet known for its playful demeanor, often characterized by whiskers on the face. Cats can be used in various ways:\n",
            "\n",
            "1. Companionship - They offer emotional support to their owners.\n",
            "2. Pest Control - Outdoor cats may help control rodent populations around homes and gardens.\n",
            "\n",
            "Additionally, they are popular pets featured as subjects of art or photography due to their distinct appearance with fur patterns unique to each individual cat (known colloquially by the human's given name).\n",
            "\n",
            "🖼️ Image: car.jpg\n",
            "📷 Recognized Label (CLIP): car\n",
            "🤖 Phi-4 Mini Response:\n",
            "This is a common type of vehicle known for its four wheels, engine-powered propulsion system that typically runs on gasoline or diesel fuel. Cars can vary greatly in terms of size from compact cars to large trucks.\n",
            "\n",
            "\n",
            "Possible uses include:\n",
            "\n",
            "- Personal transportation: Individuals usually drive them daily as part of their commute.\n",
            "  \n",
            "- Commercial transportations such as delivery services and taxis.\n",
            "\n",
            "- Recreational activities like road trips, car races (e.g., Formula One), or off-road adventures for sports vehicles.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4o2iSOVK912L",
        "outputId": "93f4e495-2855-47a3-efc6-03f75d16c411"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.31.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from PIL import Image\n",
        "from io import BytesIO\n",
        "import torch\n",
        "from transformers import CLIPProcessor, CLIPModel\n",
        "from langchain_community.llms import Ollama\n",
        "from langchain.prompts import PromptTemplate\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Download sample images\n",
        "def download_sample_images():\n",
        "    image_urls = [\n",
        "        \"https://images.unsplash.com/photo-1600585154340-be6161a56a0c\",  # House\n",
        "        \"https://images.unsplash.com/photo-1518791841217-8f162f1e1131\",  # Cat\n",
        "        \"https://images.unsplash.com/photo-1507525428034-b723cf961d3e\"   # Beach\n",
        "    ]\n",
        "    image_paths = []\n",
        "    captions = [\n",
        "        \"A modern house with large windows\",\n",
        "        \"A fluffy cat sitting on a couch\",\n",
        "        \"A tropical beach with palm trees\"\n",
        "    ]\n",
        "    os.makedirs(\"sample_images\", exist_ok=True)\n",
        "\n",
        "    for i, url in enumerate(image_urls):\n",
        "        try:\n",
        "            response = requests.get(url)\n",
        "            if response.status_code == 200:\n",
        "                img = Image.open(BytesIO(response.content))\n",
        "                img_path = f\"sample_images/image_{i+1}.jpg\"\n",
        "                img.save(img_path)\n",
        "                image_paths.append(img_path)\n",
        "                print(f\"Downloaded image {i+1} to {img_path}\")\n",
        "            else:\n",
        "                print(f\"Failed to download image {i+1}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading image {i+1}: {e}\")\n",
        "\n",
        "    return image_paths, captions\n",
        "\n",
        "# Initialize CLIP and Ollama\n",
        "def initialize_models():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(device)\n",
        "    clip_processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
        "    llm = Ollama(model=\"phi4-mini\")\n",
        "    return clip_model, clip_processor, llm, device\n",
        "\n",
        "# Use Case 1: Image Recognition (classify image based on text labels)\n",
        "def image_recognition(image_path, labels, clip_model, clip_processor, device):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    inputs = clip_processor(text=labels, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model(**inputs)\n",
        "        logits_per_image = outputs.logits_per_image\n",
        "        probs = logits_per_image.softmax(dim=1).cpu().numpy()[0]\n",
        "\n",
        "    result = {label: prob for label, prob in zip(labels, probs)}\n",
        "    predicted_label = max(result, key=result.get)\n",
        "    return predicted_label, result\n",
        "\n",
        "# Use Case 2: Image Description (generate detailed description using Phi-4 Mini)\n",
        "def image_description(image_path, clip_model, clip_processor, llm, device):\n",
        "    image = Image.open(image_path).convert(\"RGB\")\n",
        "    # Use CLIP to get a basic classification to guide the description\n",
        "    labels = [\"house\", \"cat\", \"beach\", \"car\", \"tree\"]\n",
        "    inputs = clip_processor(text=labels, images=image, return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model(**inputs)\n",
        "        logits_per_image = outputs.logits_per_image\n",
        "        probs = logits_per_image.softmax(dim=1).cpu().numpy()[0]\n",
        "        top_label = labels[np.argmax(probs)]\n",
        "\n",
        "    # Create a prompt for Phi-4 Mini\n",
        "    prompt_template = PromptTemplate(\n",
        "        input_variables=[\"label\"],\n",
        "        template=\"Generate a detailed description of an image that primarily features a {label}. Include details about possible colors, setting, and context.\"\n",
        "    )\n",
        "    prompt = prompt_template.format(label=top_label)\n",
        "\n",
        "    try:\n",
        "        description = llm.invoke(prompt)\n",
        "        return description\n",
        "    except Exception as e:\n",
        "        return f\"Error generating description: {e}\"\n",
        "\n",
        "# Use Case 3: Image Search by Text (find image matching a text query)\n",
        "def image_search_by_text(text_query, image_paths, clip_model, clip_processor, device):\n",
        "    images = [Image.open(path).convert(\"RGB\") for path in image_paths]\n",
        "    inputs = clip_processor(text=[text_query], images=images, return_tensors=\"pt\", padding=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = clip_model(**inputs)\n",
        "        logits_per_image = outputs.logits_per_image\n",
        "        probs = logits_per_image.softmax(dim=1).cpu().numpy()[0]\n",
        "\n",
        "    best_image_idx = np.argmax(probs)\n",
        "    return image_paths[best_image_idx], probs[best_image_idx]\n",
        "\n",
        "# Use Case 4: Image Search by Image (find most similar image)\n",
        "def image_search_by_image(query_image_path, image_paths, clip_model, clip_processor, device):\n",
        "    query_image = Image.open(query_image_path).convert(\"RGB\")\n",
        "    images = [Image.open(path).convert(\"RGB\") for path in image_paths]\n",
        "\n",
        "    # Encode query image\n",
        "    query_inputs = clip_processor(images=query_image, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        query_features = clip_model.get_image_features(**query_inputs).cpu().numpy()\n",
        "\n",
        "    # Encode all images\n",
        "    image_features = []\n",
        "    for image in images:\n",
        "        inputs = clip_processor(images=image, return_tensors=\"pt\").to(device)\n",
        "        with torch.no_grad():\n",
        "            features = clip_model.get_image_features(**inputs).cpu().numpy()\n",
        "        image_features.append(features)\n",
        "\n",
        "    # Compute cosine similarities\n",
        "    similarities = [\n",
        "        np.dot(query_features, feat.T) / (np.linalg.norm(query_features) * np.linalg.norm(feat))\n",
        "        for feat in image_features\n",
        "    ]\n",
        "\n",
        "    best_image_idx = np.argmax(similarities)\n",
        "    return image_paths[best_image_idx], similarities[best_image_idx]\n",
        "\n",
        "def main():\n",
        "    # Download sample images\n",
        "    image_paths, captions = download_sample_images()\n",
        "\n",
        "    # Initialize models\n",
        "    clip_model, clip_processor, llm, device = initialize_models()\n",
        "\n",
        "    # Use Case 1: Image Recognition\n",
        "    print(\"\\n=== Image Recognition ===\")\n",
        "    labels = [\"a house\", \"a cat\", \"a beach\", \"a car\", \"a tree\"]\n",
        "    for i, image_path in enumerate(image_paths):\n",
        "        predicted_label, probs = image_recognition(image_path, labels, clip_model, clip_processor, device)\n",
        "        print(f\"Image {i+1} ({image_path}):\")\n",
        "        print(f\"Predicted: {predicted_label}\")\n",
        "        print(f\"Probabilities: {probs}\")\n",
        "\n",
        "    # Use Case 2: Image Description\n",
        "    print(\"\\n=== Image Description ===\")\n",
        "    for i, image_path in enumerate(image_paths):\n",
        "        description = image_description(image_path, clip_model, clip_processor, llm, device)\n",
        "        print(f\"Image {i+1} ({image_path}):\")\n",
        "        print(f\"Description: {description}\")\n",
        "\n",
        "    # Use Case 3: Image Search by Text\n",
        "    print(\"\\n=== Image Search by Text ===\")\n",
        "    text_query = \"A tropical beach with palm trees\"\n",
        "    best_image, score = image_search_by_text(text_query, image_paths, clip_model, clip_processor, device)\n",
        "    print(f\"Text Query: '{text_query}'\")\n",
        "    print(f\"Best Match: {best_image} (Score: {score:.4f})\")\n",
        "\n",
        "    # Use Case 4: Image Search by Image\n",
        "    print(\"\\n=== Image Search by Image ===\")\n",
        "    query_image_path = image_paths[0]  # Use first image as query\n",
        "    best_match, similarity = image_search_by_image(query_image_path, image_paths, clip_model, clip_processor, device)\n",
        "    print(f\"Query Image: {query_image_path}\")\n",
        "    print(f\"Best Match: {best_match} (Similarity: {float(similarity):.4f})\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gF0ekfE1Ub7",
        "outputId": "5969a44f-8224-4547-9a26-cebc8655054e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloaded image 1 to sample_images/image_1.jpg\n",
            "Downloaded image 2 to sample_images/image_2.jpg\n",
            "Downloaded image 3 to sample_images/image_3.jpg\n",
            "\n",
            "=== Image Recognition ===\n",
            "Image 1 (sample_images/image_1.jpg):\n",
            "Predicted: a house\n",
            "Probabilities: {'a house': np.float32(0.9935417), 'a cat': np.float32(0.000102968894), 'a beach': np.float32(7.310011e-05), 'a car': np.float32(0.00032168103), 'a tree': np.float32(0.0059605595)}\n",
            "Image 2 (sample_images/image_2.jpg):\n",
            "Predicted: a cat\n",
            "Probabilities: {'a house': np.float32(0.00063693087), 'a cat': np.float32(0.9987239), 'a beach': np.float32(4.9661146e-05), 'a car': np.float32(0.00033760496), 'a tree': np.float32(0.00025181915)}\n",
            "Image 3 (sample_images/image_3.jpg):\n",
            "Predicted: a beach\n",
            "Probabilities: {'a house': np.float32(0.0013106146), 'a cat': np.float32(0.00016481701), 'a beach': np.float32(0.9975725), 'a car': np.float32(0.0004518284), 'a tree': np.float32(0.000500347)}\n",
            "\n",
            "=== Image Description ===\n",
            "Image 1 (sample_images/image_1.jpg):\n",
            "Description: Picture this: A charming little cottage stands at the edge of a quaint village nestled within rolling hills covered in lush green meadows dotted with wildflowers like lavender bluebells and purple coneflowers swaying gently to their own rhythm under an azure sky.\n",
            "\n",
            "The house itself is painted white – stark against its surroundings, making it look clean-cut yet inviting. The roof has slate shingles that gleam a bit as they catch the morning sun's rays which dance across them before spreading warmth throughout this picturesque scene of tranquility and serenity.\n",
            "\n",
            "\n",
            "Inside doors lead to quaint living rooms with wood paneling in warm tones like honey gold or rich mahogany – adorned subtly by vintage-looking furniture pieces crafted from dark-stained oak. A massive bay window on one side overlooks a small patchwork garden, brimming with vibrant flowers that add splashes of color against the backdrop.\n",
            "\n",
            "\n",
            "On another corner sits an old-fashioned fireplace - its brick walls smothered in thick layers and smoke rings curling up into it adding warmth to this humble abode's atmosphere.\n",
            "\n",
            "A cozy kitchen features white granite countertops topped by classic red tile backsplash; there, a large wooden table stands flanked with mismatched chairs. The stove is simple but functional—a testament of the home's history dating back generations.\n",
            "\n",
            "\n",
            "In front of all these inviting interiors lies an open veranda - weathered yet sturdy and draped in comfortable cushions that invite you to sit for hours while sipping on warm drinks as they sip leisurely through a crackling fire.\n",
            "\n",
            "It’s hard not wanting this house. The colors, settings, contexts – everything screams comfort wrapped inside warmth; it's like stepping into someone else's world full of love & memories etched onto every inch within the walls and floors around it - one that feels infinitely comforting to be just at home in.\n",
            "\n",
            "\n",
            "In summary: This quaint little cottage is situated amidst a charming village surrounded by lush green meadows, under an azure sky. Its white exterior with slate roof shingles stands out against its surrounding greenery making this house look inviting but still humble; within the doors leads you into warm wooden interiors adorned subtly vintage furniture pieces crafted from dark-stained oak and intricate wood paneling in honey gold or rich mahogany tones.\n",
            "\n",
            "A patchwork garden filled vibrant flowers adds splashes of color to one corner. An old-fashioned fireplace is nestled against a cozy kitchen featuring white granite countertops topped by classic red tile backsplash, with an open veranda draped comfortably inviting relaxation; it all screams comfort wrapped inside warmth making this cottage feel like stepping into someone else's world full of love & memories etched onto every inch within its walls and floors - one that feels infinitely comforting to be just at home in.\n",
            "Image 2 (sample_images/image_2.jpg):\n",
            "Description: Imagine this scene: In the center stage is a sleek silver-grey Maine Coon Cat perched regally on top of your favorite antique oak bookshelf in front of you as it gazes out at its kingdom with piercing green eyes full of mischief or wisdom depending upon how you'd like to imagine them today.\n",
            "\n",
            "The room around her radiates warmth and coziness, thanks mainly due to the vintage wallpaper adorned with floral patterns painted a soft blend of creamy yellow against an off-white backdrop. The walls are lined up by shelves filled neatly stacked books in varying shades from burnt orange on their spines to dusty rose along the side panels that catch your attention as you move around this room.\n",
            "\n",
            "The window is beautifully framed, its glass etched delicately with intricate Celtic knotwork patterns giving a hint of charm and elegance while letting through just enough sunlight for her coat's shine. A large mirror behind hangs in place beside an antique wardrobe stuffed to capacity all filled from floor up with clothes that have seen better days but smell undeniably good.\n",
            "\n",
            "The cat’s fur is smooth, glistening slightly as she stretches out on the worn-out armchair near you placed next to a dusty old gramophone spinning softly. Her ears are perked in curiosity while her tail curls neatly around them forming an adorable loop at its tip that adds just enough charm for this scene's ambiance.\n",
            "\n",
            "In conclusion - it's simple yet beautiful, filled with warmth and love as if the cat itself is aware of everything going on within it but chooses to remain peaceful amidst all.\n",
            "Image 3 (sample_images/image_3.jpg):\n",
            "Description: The central theme in this imagined scene is the tranquil beauty of a pristine sandy coastline against a backdrop where sky meets sea at dusk or dawn; the golden hues casting off clouds gradually fading into oranges as sunlight reflects on water's surface would be prominent - depicting either an orange-pink sunset with pastel shades dancing across its canvas. Surrounding elements include scattered seashells, starfish resting by waves lapping softly against pebbled shorelines and smooth beach sand blanketed in seaweed washed ashore after the tide receded.\n",
            "\n",
            "The scene is set along a secluded stretch of coastline where there are no towering structures nearby; only soft dunes gently rise behind rocky cliffs creating an unspoiled vista. Small details like scattered driftwood, children building castles from seashells or surfers riding gentle waves would enhance its picturesque ambiance adding to this idyllic seascape painting.\n",
            "\n",
            "This imagined image depicts a serene and peaceful beach setting that evokes feelings of relaxation while being surrounded by nature's beauty; the colors are soothing with pastel shades blending seamlessly into each other as sunlight slowly fades away creating warm hues against clear blue skies. Overall, it conjures an atmosphere brimming with tranquility amidst this beautiful coastal landscape painting.\n",
            "\n",
            "=== Image Search by Text ===\n",
            "Text Query: 'A tropical beach with palm trees'\n",
            "Best Match: sample_images/image_1.jpg (Score: 1.0000)\n",
            "\n",
            "=== Image Search by Image ===\n",
            "Query Image: sample_images/image_1.jpg\n",
            "Best Match: sample_images/image_1.jpg (Similarity: 1.0000)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-23a04966a4e6>:165: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  print(f\"Best Match: {best_match} (Similarity: {float(similarity):.4f})\")\n"
          ]
        }
      ]
    }
  ]
}